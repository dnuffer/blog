Today I coded up the network described in Texture Networks: Feed-forward Synthesis of Textures and Stylized Images by Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, Victor Lempitsky (https://arxiv.org/abs/1603.03417), and 
Instance Normalization: The Missing Ingredient for Fast Stylization by Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky (https://arxiv.org/abs/1607.08022), referencing torch code from https://github.com/DmitryUlyanov/texture\_nets
The pyramid architecture is very effective and made a significant quality increase.
I made a few modifications:
Use elu instead of relu.
Modified vgg to use average pooling instead of max pooling, and adjusted the style loss weight.
Both these changes improved the output.
I experimented with batch size a bit. 12 GB is not enough memory to use 32, but 16 works fine. It gives a slight performance improvement over a batch of 1.
I ran across an interesting paper that has some excellent suggestions to improve the quality: Improving the Neural Algorithm of Artistic Style by Roman Novak, Yaroslav Nikulin
The improvements, listed in order of impact, are:
* Activation shift. This is adding a constant (-1 is suggested as yielding the best result) to the vgg layer activations before computing the gram matrix.
* Using all 16 layers of vgg.
* Using a chain of "inter-layer feature correlations".
* Modifying the layer weighting scheme so that the style has higher weights on the input layers, and the content uses all the layers (instead of 1), and putting higher weights on the output layers.
